import os from "os"
import { Installation } from "@/installation"
import { Provider } from "@/provider/provider"
import { Log } from "@/util/log"
import {
  streamText,
  wrapLanguageModel,
  type ModelMessage,
  type StreamTextResult,
  type Tool,
  type ToolSet,
  extractReasoningMiddleware,
  tool,
  jsonSchema,
} from "ai"
import { clone, mergeDeep, pipe } from "remeda"
import { ProviderTransform } from "@/provider/transform"
import { Config } from "@/config/config"
import { Instance } from "@/project/instance"
import type { Agent } from "@/agent/agent"
import type { MessageV2 } from "./message-v2"
import { Plugin } from "@/plugin"
import { SystemPrompt } from "./system"
import { Flag } from "@/flag/flag"
import { PermissionNext } from "@/permission/next"
import { Auth } from "@/auth"

export namespace LLM {
  const log = Log.create({ service: "llm" })

  export const OUTPUT_TOKEN_MAX = Flag.OPENCODE_EXPERIMENTAL_OUTPUT_TOKEN_MAX || 32_000

  function isUsageV3Shape(usage: any): boolean {
    return (
      !!usage &&
      typeof usage === "object" &&
      !!usage.inputTokens &&
      typeof usage.inputTokens === "object" &&
      "total" in usage.inputTokens &&
      !!usage.outputTokens &&
      typeof usage.outputTokens === "object" &&
      "total" in usage.outputTokens
    )
  }

  // Normalize legacy usage shapes to the V3 usage schema expected by ai v6.
  function normalizeUsage(usage: any) {
    if (isUsageV3Shape(usage)) return usage

    const inputTotal = typeof usage?.inputTokens === "number" ? usage.inputTokens : undefined
    const outputTotal = typeof usage?.outputTokens === "number" ? usage.outputTokens : undefined
    const cachedInput = typeof usage?.cachedInputTokens === "number" ? usage.cachedInputTokens : undefined
    const reasoning = typeof usage?.reasoningTokens === "number" ? usage.reasoningTokens : undefined

    const noCache =
      typeof inputTotal === "number"
        ? typeof cachedInput === "number"
          ? Math.max(0, inputTotal - cachedInput)
          : inputTotal
        : undefined
    const textTokens =
      typeof outputTotal === "number"
        ? typeof reasoning === "number"
          ? Math.max(0, outputTotal - reasoning)
          : outputTotal
        : undefined

    return {
      inputTokens: {
        total: inputTotal,
        noCache,
        cacheRead: cachedInput,
        cacheWrite: undefined,
      },
      outputTokens: {
        total: outputTotal,
        text: textTokens,
        reasoning,
      },
      raw: usage?.raw,
    }
  }

  function normalizeStreamPart(part: any) {
    if (!part || part.type !== "finish") return part
    return { ...part, usage: normalizeUsage(part.usage) }
  }

  export type StreamInput = {
    user: MessageV2.User
    sessionID: string
    model: Provider.Model
    agent: Agent.Info
    system: string[]
    abort: AbortSignal
    messages: ModelMessage[]
    small?: boolean
    tools: Record<string, Tool>
    retries?: number
  }

  export type StreamOutput = StreamTextResult<ToolSet, any>

  export async function stream(input: StreamInput) {
    const l = log
      .clone()
      .tag("providerID", input.model.providerID)
      .tag("modelID", input.model.id)
      .tag("sessionID", input.sessionID)
      .tag("small", (input.small ?? false).toString())
      .tag("agent", input.agent.name)
    l.info("stream", {
      modelID: input.model.id,
      providerID: input.model.providerID,
      hasAgentPrompt: !!input.agent.prompt,
      agentPromptLength: input.agent.prompt?.length ?? 0,
      agentName: input.agent.name,
    })
    const [language, cfg, provider, auth] = await Promise.all([
      Provider.getLanguage(input.model),
      Config.get(),
      Provider.getProvider(input.model.providerID),
      Auth.get(input.model.providerID),
    ])
    const isCodex = provider.id === "openai" && auth?.type === "oauth"

    const system = SystemPrompt.header(input.model.providerID)
    system.push(
      [
        // use agent prompt otherwise provider prompt
        // For Codex sessions, skip SystemPrompt.provider() since it's sent via options.instructions
        ...(input.agent.prompt ? [input.agent.prompt] : isCodex ? [] : SystemPrompt.provider(input.model)),
        // persona-specific system prompt additions (from AgentPersonaConfig)
        ...(input.agent.systemPromptAdditions ? [input.agent.systemPromptAdditions] : []),
        // any custom prompt passed into this call
        ...input.system,
        // any custom prompt from last user message
        ...(input.user.system ? [input.user.system] : []),
      ]
        .filter((x) => x)
        .join("\n"),
    )

    const header = system[0]
    const original = clone(system)
    // For Anthropic: system[0] is header, system[1] is content
    // For others: system[0] is content (no separate header)
    const mainContent = system.length > 1 ? system[1] : system[0]
    // Enhanced logging for persona debugging - log at info level for visibility
    l.info("system prompt constructed", {
      systemParts: system.length,
      headerLength: header?.length ?? 0,
      mainContentLength: mainContent?.length ?? 0,
      agentPromptLength: input.agent.prompt?.length ?? 0,
      agentPromptPreview: input.agent.prompt?.slice(0, 100) ?? "(no prompt)",
      includesAgentPrompt: input.agent.prompt ? mainContent?.includes(input.agent.prompt.slice(0, 50)) : false,
      systemContentPreview: mainContent?.slice(0, 200) ?? "(no content)",
    })
    await Plugin.trigger("experimental.chat.system.transform", { sessionID: input.sessionID }, { system })
    if (system.length === 0) {
      system.push(...original)
    }
    // rejoin to maintain 2-part structure for caching if header unchanged
    if (system.length > 2 && system[0] === header) {
      const rest = system.slice(1)
      system.length = 0
      system.push(header, rest.join("\n"))
    }

    const variant =
      !input.small && input.model.variants && input.user.variant ? input.model.variants[input.user.variant] : {}
    const base = input.small
      ? ProviderTransform.smallOptions(input.model)
      : ProviderTransform.options({
          model: input.model,
          sessionID: input.sessionID,
          providerOptions: provider.options,
        })
    // Filter out non-provider options (like 'includes' which is for skill loading)
    const agentProviderOptions = { ...input.agent.options }
    delete agentProviderOptions.includes
    const options: Record<string, any> = pipe(
      base,
      mergeDeep(input.model.options),
      mergeDeep(agentProviderOptions),
      mergeDeep(variant),
    )
    if (isCodex) {
      options.instructions = SystemPrompt.instructions()
    }

    const params = await Plugin.trigger(
      "chat.params",
      {
        sessionID: input.sessionID,
        agent: input.agent,
        model: input.model,
        provider,
        message: input.user,
      },
      {
        temperature: input.model.capabilities.temperature
          ? (input.agent.temperature ?? ProviderTransform.temperature(input.model))
          : undefined,
        topP: input.agent.topP ?? ProviderTransform.topP(input.model),
        topK: input.agent.topK ?? ProviderTransform.topK(input.model),
        // Additional sampling parameters from agent config
        frequencyPenalty: input.agent.frequencyPenalty,
        presencePenalty: input.agent.presencePenalty,
        seed: input.agent.seed,
        options,
      },
    )

    // Enhanced parameter logging for debugging
    l.info("stream params", {
      temperature: params.temperature,
      temperatureSource: input.agent.temperature !== undefined ? "agent" : "model",
      topP: params.topP,
      topPSource: input.agent.topP !== undefined ? "agent" : "model",
      topK: params.topK,
      frequencyPenalty: params.frequencyPenalty,
      presencePenalty: params.presencePenalty,
      seed: params.seed,
      variant: input.user.variant ?? "default",
      thinkingBudget: params.options?.thinkingBudget ?? params.options?.thinking?.budget,
      reasoningEffort: params.options?.reasoningEffort ?? params.options?.reasoning_effort,
    })
    l.debug("stream options", { options: params.options })

    const maxOutputTokens = isCodex
      ? undefined
      : ProviderTransform.maxOutputTokens(
          input.model.api.npm,
          params.options,
          input.model.limit.output,
          OUTPUT_TOKEN_MAX,
        )

    const tools = await resolveTools(input)

    // LiteLLM and some Anthropic proxies require the tools parameter to be present
    // when message history contains tool calls, even if no tools are being used.
    // Add a dummy tool that is never called to satisfy this validation.
    // This is enabled for:
    // 1. Providers with "litellm" in their ID or API ID (auto-detected)
    // 2. Providers with explicit "litellmProxy: true" option (opt-in for custom gateways)
    const isLiteLLMProxy =
      provider.options?.["litellmProxy"] === true ||
      input.model.providerID.toLowerCase().includes("litellm") ||
      input.model.api.id.toLowerCase().includes("litellm")

    if (isLiteLLMProxy && Object.keys(tools).length === 0 && hasToolCalls(input.messages)) {
      tools["_noop"] = tool({
        description:
          "Placeholder for LiteLLM/Anthropic proxy compatibility - required when message history contains tool calls but no active tools are needed",
        inputSchema: jsonSchema({ type: "object", properties: {} }),
        execute: async () => ({ output: "", title: "", metadata: {} }),
      })
    }

    return streamText({
      onError(error) {
        l.error("stream error", {
          error,
        })
      },
      async experimental_repairToolCall(failed) {
        const lower = failed.toolCall.toolName.toLowerCase()
        if (lower !== failed.toolCall.toolName && tools[lower]) {
          l.info("repairing tool call", {
            tool: failed.toolCall.toolName,
            repaired: lower,
          })
          return {
            ...failed.toolCall,
            toolName: lower,
          }
        }
        return {
          ...failed.toolCall,
          input: JSON.stringify({
            tool: failed.toolCall.toolName,
            error: failed.error.message,
          }),
          toolName: "invalid",
        }
      },
      // Only include sampling parameters if defined - some providers (Google) reject undefined values
      ...(params.temperature !== undefined && { temperature: params.temperature }),
      ...(params.topP !== undefined && { topP: params.topP }),
      ...(params.topK !== undefined && { topK: params.topK }),
      ...(params.frequencyPenalty !== undefined && { frequencyPenalty: params.frequencyPenalty }),
      ...(params.presencePenalty !== undefined && { presencePenalty: params.presencePenalty }),
      ...(params.seed !== undefined && { seed: params.seed }),
      providerOptions: ProviderTransform.providerOptions(input.model, params.options),
      activeTools: Object.keys(tools).filter((x) => x !== "invalid" && x !== "_noop"),
      tools,
      maxOutputTokens,
      abortSignal: input.abort,
      headers: {
        ...(isCodex
          ? {
              originator: "opencode",
              "User-Agent": `opencode/${Installation.VERSION} (${os.platform()} ${os.release()}; ${os.arch()})`,
              session_id: input.sessionID,
            }
          : undefined),
        ...(input.model.providerID.startsWith("opencode")
          ? {
              "x-opencode-project": Instance.project.id,
              "x-opencode-session": input.sessionID,
              "x-opencode-request": input.user.id,
              "x-opencode-client": Flag.OPENCODE_CLIENT,
            }
          : input.model.providerID !== "anthropic"
            ? {
                "User-Agent": `opencode/${Installation.VERSION}`,
              }
            : undefined),
        ...input.model.headers,
      },
      maxRetries: input.retries ?? 3, // Default to 3 retries for transient failures (timeouts, 503, 429)
      messages: [
        ...(isCodex
          ? [
              {
                role: "user",
                content: system.join("\n\n"),
              } as ModelMessage,
            ]
          : system.map(
              (x): ModelMessage => ({
                role: "system",
                content: x,
              }),
            )),
        ...input.messages,
      ],
      model: wrapLanguageModel({
        // @ts-expect-error - LanguageModel type mismatch between @ai-sdk/provider versions
        model: language,
        middleware: [
          {
            specificationVersion: "v3" as const,
            async transformParams(args) {
              if (args.type === "stream") {
                // @ts-expect-error
                args.params.prompt = ProviderTransform.message(args.params.prompt, input.model, options)
              }
              return args.params
            },
          },
          {
            specificationVersion: "v3" as const,
            async wrapGenerate({ doGenerate }) {
              const result = await doGenerate()
              return { ...result, usage: normalizeUsage(result.usage) }
            },
            async wrapStream({ doStream }) {
              const result = await doStream()
              return {
                ...result,
                stream: result.stream.pipeThrough(
                  new TransformStream({
                    transform(part, controller) {
                      controller.enqueue(normalizeStreamPart(part))
                    },
                  }),
                ),
              }
            },
          },
          extractReasoningMiddleware({ tagName: "think", startWithReasoning: false }),
        ],
      }),
      experimental_telemetry: { isEnabled: cfg.experimental?.openTelemetry },
    })
  }

  async function resolveTools(input: Pick<StreamInput, "tools" | "agent" | "user">) {
    const disabled = PermissionNext.disabled(Object.keys(input.tools), input.agent.permission)
    for (const tool of Object.keys(input.tools)) {
      if (input.user.tools?.[tool] === false || disabled.has(tool)) {
        delete input.tools[tool]
      }
    }
    return input.tools
  }

  // Check if messages contain any tool-call content
  // Used to determine if a dummy tool should be added for LiteLLM proxy compatibility
  export function hasToolCalls(messages: ModelMessage[]): boolean {
    for (const msg of messages) {
      if (!Array.isArray(msg.content)) continue
      for (const part of msg.content) {
        if (part.type === "tool-call" || part.type === "tool-result") return true
      }
    }
    return false
  }
}
